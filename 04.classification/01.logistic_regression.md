# 04-1. 로지스틱 회귀

## Lcture
- https://www.youtube.com/watch?v=pO27UnTsYQU


## 럭키백 : K-최근접 이웃의 다중 분류 (p177~p182)

```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()


fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()


from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)


from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)


from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))

print(kn.classes_)
print(kn.predict(test_scaled[:5]))

import numpy as np

proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```

- `proba = kn.predict_proba(test_scaled[:5])`
  - 5개 샘플에 대한 7종 생선일 각 확률을 표현



## 로지스틱 회귀 (p183~)
선형 회귀와 비슷한, 머신러닝의 가장 기본적인 분류 모델

### 시그모이드 함수(=로지스틱 함수)
공식의 결과값이 (-)무한대에서 (+)무한대까지로 나오지 않도록, 0~1 범위의 결과가 나오도록 해주는 함수


## 로지스특 회귀(이진 분류) (p185~

- `bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')` 이런 것을 boolean indexing이라고 한다.

```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

print(lr.predict(train_bream_smelt[:5]))
print(lr.predict_proba(train_bream_smelt[:5]))
```

- `print(lr.predict_proba(train_bream_smelt[:5]))`
  - 5개의 샘플에 대해서 1열은 음성, 2열은 양성으로 해서 확률을 보여준다.

- 로지스틱 회귀 계수 확인

```python
print(lr.coef_, lr.intercept_)
```

- 결과 값이 아래와 같이 나오는데,
```
[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
```
- 그 의미는 아래와 같다.
```
z = -0.404x무게 - 0.576x길이 - 0.663x대각선 - 0.013x높이 - 0.732x두께 - 2.161
```

- z값을 출력해보자

```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)

from scipy.special import expit

print(expit(decisions))
```

- `lr.decision_function()`이 z값을 출력하라는 명령어다
- `expit()`은 시그노이드 함수이다.
  - 앞에서 확인한 `lr.predict_proba()`의 양성 확률과 같은 결과를 보인다.


## 로지스특 회귀(다중 분류) (p188~)
앞에서 살펴본 것은 `이진 분류` 이번에는 `다중`

```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

print(lr.coef_.shape, lr.intercept_.shape)
```

- L2 노룸 규제 기본 적용되어 있는데, 앞에서는 alpha가 쓰여졌지만 여기에서는 `C`를 사용한다.
  - `C` 값은 커지면 `alpha`와 달리 규제가 약해지고, 작아지면 규제가 커진다.
  - `C` 값의 기본값은 `1`이다.

- `proba = lr.predict_proba(test_scaled[:5])` 5개 샘플에 대한 7종 생선의 확률을 보여준다.


### 소프트맥스 함수

```python
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))

from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

- `lr.decision_function()`은 z값을 그대로 출력하는 것이다.
  - 그래서 결과는 7x5로 나온다.

- `softmax()`는 지수함수로 시그모이드 결과를 보여준다.
