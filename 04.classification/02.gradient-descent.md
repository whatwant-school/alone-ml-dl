# 04-2 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
교재 p199~


## Lecture
- https://www.youtube.com/watch?v=A1UUnfijQfQ


## 배경
- 데이터가 추가되었을 때, 기존 모델을 버리고 새로 학습하는 것이 아니라 업데이트를 하면 안될까?
- = 점진적 학습 (온라인 학습도 점진적 학습 방법 중 하나)
- → 확률적 경사 하강법 (머신러닝 알고리즘이 아니라, 훈련하는 방법 중 하나. 최적화 방법)


## 확률적 경사 하강법 (확률적 = 랜덤)


### 훈련 : 조금씩 경사를 따라 이동
훈련세트에서 샘플을 꺼내서 확인하는 과정

- 확률적 경사 하강법 : 한 개씩 꺼내기
- 미니 배치 경사 하강법 : 여러 개씩 꺼내기 (개수: 하이퍼 파라미터)
- 배치 경사 하강법 : 몽땅 꺼내기


### 에포크 (Epoch)
훈련 세트를 모두 다 사용했다 = 1 에포크


### 손실 함수
나쁜 정도를 측정하는 함수

- 손실 함수의 결과가 작아지도록 훈련
- 분류에서의 정확도는 미분 가능하지 않다 (연속적이지 않기 때문에) → 로지스틱 손실 함수


### 로지스틱 손실 함수 (이진 크로스 엔트로피 손실 함수)
회귀 → (평균 절댓값 오차/평균 제곱 오차) = 미분 가능 → 손실 함수 = 측정 지표


### pre-processing
경사하강법을 적용하기 위해서는 스케일을 맞춰줘야 한다

- 반드시 `StandardScaler`로 해줘야 한다.

```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()


from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)


from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


## SGDClassifier
분류 모델에 대해서 먼저 살펴보자

```python
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

- 회귀였다면, `SGDRegressor`를 사용하면 된다.
- `loss='log'` : 로지스틱 회귀 모델을 사용하라는 의미
  - SGD는 머신러닝 모델이 아니라 최적화 하는 방법이기 때문에 어떤 모델을 최적화 할 것인지 지정해줘야 함
- `max_iter=10` : 에포크와 같은 의미

- `sc.partial_fit(train_scaled, train_target)` : 기존 w, b를 유지하면서 훈련
  - `fit`을 사용하면 기존 학습된 내용을 버리고 새로 하는 것

- 한 번 더 훈련한 결과가 더 잘 나온 것을 확인할 수 있다.


### 에포크와 과대/과소 적합
- 에포크가 작으면 과소 적합 → 에포크가 커지면 과대 적합이 될 수 있다.
- 중간 정도에서 자동으로 멈추도록 하는 것 = 조기 종료


### 조기 종료

```python
import numpy as np

sc = SGDClassifier(loss='log', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))

import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

- `for문`의 이유는 그래프 그리려고...
- test 결과가 떨어지는 것이 극적으로 보이지는 않지만 epoch 100 정도에서 조기 종료되어야 하는 것은 보인다.

```python
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

- 그래서 위와 같이 100 정도의 값을 주면 된다는 것을 알 수 있다.
