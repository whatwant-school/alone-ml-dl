# 03-03. 특성 공학과 규제


## Lecture
- https://www.youtube.com/watch?v=PLECEclz0p4


## 다중 회귀 (multiple/multinomial regression) - p151
- 복수 특성을 이용한 회귀 (2차원, n차원)

### Feature Engineering (특성 공학)
- 제곱을 이용해서 특성을 추가하거나 하는 행위
- 머신러닝은 특성 공학의 영향을 많이 받지만,
- 딥러닝은 비교적 영향이 적다.


## 판다스로 데이터 준비 - p152
- CSV -> pd.read_csv() -> to_numpy()

```python
import pandas as pd

df = pd.read_csv('https://bit.ly/perch_csv')
perch_full = df.to_numpy()

print(perch_full)
```


## 다항 특성 만들기 - p154
- PolynomialFeature : 변환기 (Transformer)
  - fit -> transform = fit_transform()
- LinearRegression / KNeighbor... : 추정기 (Estimator)
  - fit -> predict -> score

```python
from sklearn.preprocessing import PolynomialFeatures

# defree=2
poly = PolynomialFeatures()
poly.fit([[2, 3]])

# 1(bias), 2, 3, 2**2, 2*3, 3**2
# 2, 3은 입력한 그대로, 절편으로 1 추가, 2의 제곱, 3의 제곱, 2와3의 곱
print(poly.transform([[2, 3]]))
```

## LinearRegression - p156
- 특성을 더 만들어 넣어줘서 정확성을 올리는 과정

```python
# degree = 2
# 자동으로 절편(가중치) 1값이 추가되는 것을 막기 위해 False
poly = PolynomialFeatures(include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)

# (42, 9) -> 42개의 행이 있다는 것을 알 수 있다.
print(train_poly.shape)

# 각 항목의 제곱과 항목끼리의 곱으로 구성되었음을 볼 수 있다. (degree=2 이기에?!)
poly.get_feature_names()

# train을 변환했으면 test도 변환해야 한다.
test_poly = poly.transform(test_input)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly, train_target)

print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))
```


## 더 많은 특성 만들기 - p157
- 특성을 늘려서 학습을 시키면 더 좋아지지 않을까?!

```python
# degree 값을 이용해서 특성을 늘릴 수 있다.
poly = PolynomialFeatures(degree=5, include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

# degree = 5 값을 줬기에 특성이 늘어났다.
# (42, 55) -> 55개의 특성이 만들어졌다.
print(train_poly.shape)

lr.fit(train_poly, train_target)

print(lr.score(train_poly, train_target))

# -144.4057..... -> 과대 적합이 되어서 test 결과가 이렇게 나왔다.
# 42개 데이터 밖에 없는데, 특성이 55개이기에 -144가 나올 수 밖에 없다.
print(lr.score(test_poly, test_target))
```


## 규제 전에 표준화 - p159
- 과대적합이 되는 것을 막기 위한 것 = 규제 (`정규화`라고도 하는데, nomalize는 아니기에 저자는 `규제`라고 번역했단다)
- 규제 대표적인 방법 : 릿지 회귀, 라쏘 회귀
- 규제를 하기 전에 특성의 스케일이 비슷해야, 기울기도 비슷해진다.
  - Regression은 스케일에 영향을 적게 받기에 신경쓰지 않고 적용 가능하지만
  - 규제는 영향을 많이 받기에 표준화를 미리 해줘야 한다.
- 이전 챕터에서 직접했던 표준점수를 이번에는 sklearn을 이용해서 해보자.

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

## 릿지 회귀 - p160
- 가중치를 높이면 벌칙을 받기 때문에, 가중치를 줄이려고 한다. 그래서 규제
- 가중치의 제곱을 기준으로 벌칙 부여 (그래서 `L2 규제`라고도 한다)

```python
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))

print(ridge.score(test_scaled, test_target))
```

- `ridge = Ridge()`에서 벌칙의 강도를 조절할 수 있는 매개변수가 있다 (alpha)
  - default 값은 `1`이다.
  - `ridge = Ridge(alpha=5)`와 같이 조정할 수 있다.
  - 이러한 것을 `hyper parameter`라고 부른다.


## 적절한 규제 강도 찾기

```python
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]

for alpha in alpha_list:
  ridge = Ridge(alpha=alpha)
  ridge.fit(train_scaled, train_target)
  
  train_score.append(ridge.score(train_scaled, train_target))
  test_score.append(ridge.score(test_scaled, test_target))

# 그래프 범위를 맞춰주기 위해서 (0.001, 0.01, 0.1 등의 간격은 좁고, 뒷부분은 간격이 넓기에 log 함수로...)
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()

# 그래프 왼쪽은 과대 적합, 오른쪽은 과서 적합

ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
  
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```


## 라쏘 회귀 - p163
- 가중치의 절대값을 벌칙으로 준다. 그래서 `L1` 규제라고도 한다.

```python
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
  
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

# alpha
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
  
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

# 55개 특성을 넣었는데, 40개를 사용하지 않는다는 것을 알 수 있다
# 특성x가중치 인데, 가중치를 0으로 준다는 것은 특성을 사용하지 않는다는 것
print(np.sum(lasso.coef_ == 0))
```
