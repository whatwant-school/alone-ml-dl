# 03-03. 특성 공학과 규제


## Lecture

  - //


## 다중 회귀 (multiple/multinomial regression) - p151

  - 복수 특성을 이용한 회귀


## 판다스로 데이터 준비 - p152

  - CSV -> pd.read_csv() -> to_numpy()

```python
import pandas as pd

df = pd.read_csv('https://bit.ly/perch_csv')
perch_full = df.to_numpy()

print(perch_full)
```


## 다항 특성 만들기 - p154

  - PolynomialFeature : 변환기 (Transformer)
    - fit -> transform = fit_transform()
  - LinearRegression / KNeighbor... : 추정기 (Estimator)
    - fit -> predict -> score

```python
from sklearn.preprocessing import PolynomialFeature

# defree=2
poly = PolynomialFeatures()
poly.fit([[2, 3]])

# 1(bias), 2, 3, 2**2, 2*3, 3**2
print(poly.transform([[2, 3]]))
```

## LinearRegression - p156

  - 특성을 더 만들어 넣어줘서 정확성을 올리는 과정

```python
# degree = 2
poly = PolynomialFeatures(include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)

print(train_poly.shape)

poly.get_feature_names()

test_poly = poly.transform(test_input)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly, train_target)

print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))
```


## 더 많은 특성 만들기 - p157

  - 과대적합이 되는 것을 막기 위한 것 = 규제 (`정규화`라고도 하는데, nomalize는 아니기에 저자는 `규제`라고 번역했단다)

```python
poly = PolynomialFeatures(degree=5, include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

print(train_poly.shape)
# (42, 55) -> 55개의 특성이 만들어졌다.

lr.fit(train_poly, train_target)

print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))
# -144.4057..... -> 과대 적합이 되어서 test 결과가 이렇게 나왔다.
```


## 규제 전에 표준화 - p159

  - 특성의 스케일이 비슷해야, 기울기도 비슷해진다.
  - 이전 챕터에서 직접했던 표준점수를 이번에는 sklearn을 이용해서 해보자.

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

## 릿지 회귀 - p160

  - 가중치의 제곱을 기준으로 벌칙 부여 (그래서 `L2 규제`라고도 한다)

```python
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))

print(ridge.score(test_scaled, test_target))
```

  - `ridge = Ridge()`에서 벌칙의 강도를 조절할 수 있는 매개변수가 있다 (alpha)
    - default 값은 `1`이다.
    - `ridge = Ridge(alpha=5)`와 같이 조정할 수 있다.
    - 이러한 것을 `hyper parameter`라고 부른다.


## 적절한 규제 강도 찾기

```python
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]

for alpha in alpha_list:
  ridge = Ridge(alpha=alpha)
  ridge.fit(train_scaled, train_target)
  
  train_score.append(ridge.score(train_scaled, train_target))
  test_score.append(ridge.score(test_scaled, test_target))

# 그래프 범위를 맞춰주기 위해서
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()

ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
  
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```


## 라쏘 회귀 - p163

  - `L1` 규제라고도 한다.

```python
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
  
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
  
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

print(np.sum(lasso.coef_ == 0))
## 55개 특성을 넣었는데, 40개를 사용하지 않는다는 것을 알 수 있다
```
